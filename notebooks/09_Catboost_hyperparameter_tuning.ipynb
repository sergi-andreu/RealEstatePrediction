{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost hyperparameter tuning\n",
    "\n",
    "To find better hyperparameters than the default, we can do grid search / some search over 'good' combination of parameters, and select the best as with respect to some metric.\n",
    "\n",
    "We do this with [Weights&Biases](https://wandb.a). If you need access to that, let me know.\n",
    "\n",
    "However, this work has been a bit useless.\n",
    "\n",
    "I rushed into hyperparameter tuning toooo early, and should have tried some feature elimination first.\n",
    "\n",
    "The final model does not use the best parameters by this notebook (although they are inspired on this)<br>\n",
    "**One should repeat the process of this notebook for the final model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd()+ \"/../\")\n",
    "from src.data_preprocessing import DataPreprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the features\n",
    "The datapreprocessing pipeline is doing quite some stuff, and in a non-efficient manner (I don't have much time for optimizing that :( )\n",
    "But it should be less than 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPreprocessing(df_path = \"../data/real_estate_ads_2022_10.csv\",\n",
    "                        train_indices_path=\"../data/train_indices.npy\", \n",
    "                        test_indices_path=\"../data/test_indices.npy\",\n",
    "                        get_params_from_params=True,\n",
    "                        get_tfidf_embeddings_flag=True,\n",
    "                        get_bert_embeddings_flag=True,\n",
    "                        get_textual_features_flag=True,\n",
    "                        transform_time_features_flag=True,\n",
    "                        transform_cyclic_features_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the metrics class\n",
    "That is convenient to compute multiple metrics:\n",
    "- **explained_variance_score**: Measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A score of 1 indicates perfect prediction, while a score of 0 indicates that the model does not explain any of the variance.\n",
    "\n",
    "- **r2_score**: Also known as the coefficient of determination, it indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A value of 1 indicates a perfect fit, while a value of 0 indicates that the model does not explain any of the variance.\n",
    "\n",
    "- **mean_absolute_percentage_error (MAPE)**: Measures the average of the absolute percentage errors of predictions. It provides a percentage error which is easy to interpret but can be sensitive to very small actual values.\n",
    "\n",
    "- **median_absolute_error**: Computes the median of all absolute differences between the target and predicted values. This metric is robust to outliers and gives a better sense of the typical error when outliers are present.\n",
    "\n",
    "- **mean_squared_error (MSE)**: Measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. It penalizes larger errors more than smaller ones due to squaring.\n",
    "\n",
    "- **mean_squared_log_error (MSLE)**: Similar to MSE but takes the logarithm of the predictions and actual values. It is useful when you want to penalize underestimation more than overestimation and is less sensitive to large errors than MSE.\n",
    "\n",
    "- **custom metrics**: Compute the percentage of times that the error falls less than some threshold. This may correlate with customer satisfaction, if they are for example happy if there's less than a 5% rate, this would count the percentage of happy customers. Of course, this will need further study (for example, segmenting the score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.compute_metrics\n",
    "importlib.reload(src.compute_metrics) # We do this for debugging purposes\n",
    "\n",
    "from src.compute_metrics import Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train / test data\n",
    "We can use the datapreprocessing method for that.\n",
    "\n",
    "This is done for better reproducibility, but can be done with the sklearn train / test split, and setting a seed should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = dp.get_train_test_split(dp.X)\n",
    "y_train, y_test = dp.get_train_test_split(dp.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define convenience functions for prettier display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_metrics(metrics_dict, only_validation=True, format_mean_std_together=True):\n",
    "\n",
    "    if only_validation:\n",
    "        metrics_dict = {key: value for key, value in metrics_dict.items() if \"test_\" in key}\n",
    "\n",
    "    if format_mean_std_together:\n",
    "        metrics_dict = {key: f\"{value:.2f} ± {metrics_dict[key+'_std']:.2f}\" for key, value in metrics_dict.items() if \"std\" not in key}\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.replace(\"nan ± nan\", \"0\").apply(lambda x: x.split(\"+-\")[0]).max()\n",
    "    return ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.replace(\"nan ± nan\", \"0\").apply(lambda x: x.split(\"+-\")[0]).min()\n",
    "    return ['font-weight: bold' if v else '' for v in is_min]\n",
    "\n",
    "def format_results_df(results, column_names=None):\n",
    "    results_df = pd.DataFrame(results).T\n",
    "\n",
    "    if column_names is not None:\n",
    "        results_df.columns = column_names\n",
    "    \n",
    "    def apply_highlight(column):\n",
    "        if column.name in [\"test_explained_variance\", \"test_r2\", \"test_custom_1\", \"test_custom_5\", \"test_custom_10\", \"test_custom_20\"]:\n",
    "            return highlight_max(column)\n",
    "        else:\n",
    "            return highlight_min(column)\n",
    "\n",
    "    \n",
    "    return results_df.style.apply(apply_highlight, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some hyperparameter tuning\n",
    "\n",
    "We can use Weights&Biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msergi-andreu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "def wandb_train(config=None):\n",
    "\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        bst = CatBoostRegressor(**config)\n",
    "\n",
    "        metrics = Metrics(dp=dp, backward_transform_flag=True, backward_standardize_flag=False)\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_index, test_index in kf.split(X, y):\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            bst.fit(X_train, y_train)\n",
    "            y_pred = bst.predict(X_val)\n",
    "                \n",
    "            computed_metrics = metrics.get_single_train_val_metrics(bst, X_train, y_train, X_val, y_val)\n",
    "            metrics.append(computed_metrics)\n",
    "\n",
    "        average_metrics = metrics.get_average()\n",
    "        std_metrics = metrics.get_std()\n",
    "        # Add _std to the keys to differentiate them from the average metrics:\n",
    "        std_metrics = {f\"{key}_std\" : value for key, value in std_metrics.items()} \n",
    "\n",
    "        wandb.log({**average_metrics, **std_metrics})\n",
    "\n",
    "    return {**average_metrics, **std_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: pvw4pbc7\n",
      "Sweep URL: https://wandb.ai/sergi-andreu/olx/sweeps/pvw4pbc7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'method': 'bayes',\n",
       " 'metric': {'name': 'test_custom_5', 'goal': 'maximize'},\n",
       " 'parameters': {'iterations': {'values': [5000]},\n",
       "  'learning_rate': {'distribution': 'log_uniform_values',\n",
       "   'min': 0.005,\n",
       "   'max': 0.1},\n",
       "  'depth': {'values': [10, 15]},\n",
       "  'subsample': {'distribution': 'uniform', 'min': 0.9, 'max': 1},\n",
       "  'colsample_bylevel': {'values': [1]},\n",
       "  'min_data_in_leaf': {'values': [1]}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a method for hyperparameter optimization\n",
    "# It can be grid search, random, or bayesian search\n",
    "\n",
    "# We use grid search. The reason being that, although it is more computationally expensive,\n",
    "# it is very effective, and would not create noise / overfitting in the hyperparameter search\n",
    "\n",
    "# It is best for a first exploration\n",
    "\n",
    "sweep_config = {\n",
    "    'method' : 'bayes'\n",
    "}\n",
    "\n",
    "# The metrics would not be used (since using grid search)\n",
    "# Just adding in case we want to use another method later\n",
    "\n",
    "metric = {\n",
    "    'name' : 'test_custom_5',\n",
    "    'goal' : 'maximize'\n",
    "}\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "\"\"\"\n",
    "# Now the important part: the parameters to sweep, for a catboost classifier:\n",
    "parameters_dict = {\n",
    "    'iterations': { # Number of iterations\n",
    "        'values' : [1000, 5000] # [100, 200, 500]\n",
    "    },\n",
    "    'learning_rate' : { # Learning rate\n",
    "        'values' : [0.005, 0.01, 0.05, 0.1, 0.022760000079870224] # [0.001, 0.01, 0.1]\n",
    "    },\n",
    "    'depth' : { # Depth of the tree\n",
    "        'values' :  [4, 6, 8] # [1, 5, 10]\n",
    "    },\n",
    "    'subsample' : { # Subsample ratio\n",
    "        'values' : [0.800000011920929, 1] #[0.05, 0.5, 1]\n",
    "    },\n",
    "    'colsample_bylevel' : { # Column subsample ratio\n",
    "        'values' : [1] #[0.05, 0.5, 1]\n",
    "    },\n",
    "    'min_data_in_leaf' : { # Minimum number of data in leaf\n",
    "        'values' : [1, 20, 50] #[1, 5, 20, 50, 100]\n",
    "    },\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "parameters_dict = {\n",
    "    'iterations': { # Number of iterations\n",
    "        'values' : [1000, 2000]\n",
    "    },\n",
    "    'learning_rate' : { # Learning rate\n",
    "        'distribution' : 'log_uniform_values',\n",
    "        'min' : 0.005,\n",
    "        'max' : 0.1\n",
    "    },\n",
    "    'depth' : { # Depth of the# [1, 5, 10]\n",
    "        'values' :  [2, 5, 6, 10] \n",
    "    },\n",
    "    'subsample' : { # Subsample ratio\n",
    "        'distribution' : 'uniform',\n",
    "        'min' : 0.8,\n",
    "        'max' : 1\n",
    "    },\n",
    "    'colsample_bylevel' : { # Column subsample ratio\n",
    "        'values' : [0.5, 1] #[0.05, 0.5, 1]\n",
    "    },\n",
    "    'min_data_in_leaf' : { # Minimum number of data in leaf\n",
    "        'values' : [1, 20] #[1, 5, 20, 50, 100]\n",
    "    },\n",
    "}\n",
    "\n",
    "parameters_dict = {\n",
    "    'iterations': { # Number of iterations\n",
    "        'values' : [5000]\n",
    "    },\n",
    "    'learning_rate' : { # Learning rate\n",
    "        'distribution' : 'log_uniform_values',\n",
    "        'min' : 0.005,\n",
    "        'max' : 0.1\n",
    "    },\n",
    "    'depth' : { # Depth of the# [1, 5, 10]\n",
    "        'values' :  [10, 15] \n",
    "    },\n",
    "    'subsample' : { # Subsample ratio\n",
    "        'distribution' : 'uniform',\n",
    "        'min' : 0.9,\n",
    "        'max' : 1\n",
    "    },\n",
    "    'colsample_bylevel' : { # Column subsample ratio\n",
    "        'values' : [1] #[0.05, 0.5, 1]\n",
    "    },\n",
    "    'min_data_in_leaf' : { # Minimum number of data in leaf\n",
    "        'values' : [1] #[1, 5, 20, 50, 100]\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config[\"parameters\"] = parameters_dict\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"olx\")\n",
    "\n",
    "sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "wandb.agent(sweep_id, function=wandb_train);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
