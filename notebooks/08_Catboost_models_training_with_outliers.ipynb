{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost training\n",
    "\n",
    "We train CatBoost models here, using all the previous knowledge that we got.\n",
    "\n",
    "Why CatBoost? Formally:\n",
    "\n",
    "- Catboost has several advantages over xgboost:\n",
    "    - Requires less hyperparameter tuning (but also has less flexibility)\n",
    "    - It usually works better 'out of the box'\n",
    "    - Can deal with categorical features without the need for one-hot encoding\n",
    "    - It usually works best with categorical variables (xgboost with numerical)\n",
    "    - We can try it out with different configurations on how to load the data\n",
    "\n",
    "Informally: it usually works fine and I like cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd()+ \"/../\")\n",
    "from src.data_preprocessing import DataPreprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all the features\n",
    "The datapreprocessing pipeline is doing quite some stuff, and in a non-efficient manner (I don't have much time for optimizing that :( )\n",
    "But it should be less than 2 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataPreprocessing(df_path = \"../data/real_estate_ads_2022_10.csv\",\n",
    "                        train_indices_path=\"../data/train_indices.npy\", \n",
    "                        test_indices_path=\"../data/test_indices.npy\",\n",
    "                        get_params_from_params=True,\n",
    "                        get_tfidf_embeddings_flag=True,\n",
    "                        get_bert_embeddings_flag=True,\n",
    "                        get_textual_features_flag=True,\n",
    "                        transform_time_features_flag=True,\n",
    "                        transform_cyclic_features_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the metrics class\n",
    "That is convenient to compute multiple metrics:\n",
    "- **explained_variance_score**: Measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A score of 1 indicates perfect prediction, while a score of 0 indicates that the model does not explain any of the variance.\n",
    "\n",
    "- **r2_score**: Also known as the coefficient of determination, it indicates the proportion of the variance in the dependent variable that is predictable from the independent variable(s). A value of 1 indicates a perfect fit, while a value of 0 indicates that the model does not explain any of the variance.\n",
    "\n",
    "- **mean_absolute_percentage_error (MAPE)**: Measures the average of the absolute percentage errors of predictions. It provides a percentage error which is easy to interpret but can be sensitive to very small actual values.\n",
    "\n",
    "- **median_absolute_error**: Computes the median of all absolute differences between the target and predicted values. This metric is robust to outliers and gives a better sense of the typical error when outliers are present.\n",
    "\n",
    "- **mean_squared_error (MSE)**: Measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. It penalizes larger errors more than smaller ones due to squaring.\n",
    "\n",
    "- **mean_squared_log_error (MSLE)**: Similar to MSE but takes the logarithm of the predictions and actual values. It is useful when you want to penalize underestimation more than overestimation and is less sensitive to large errors than MSE.\n",
    "\n",
    "- **custom metrics**: Compute the percentage of times that the error falls less than some threshold. This may correlate with customer satisfaction, if they are for example happy if there's less than a 5% rate, this would count the percentage of happy customers. Of course, this will need further study (for example, segmenting the score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.compute_metrics\n",
    "importlib.reload(src.compute_metrics) # We do this for debugging purposes\n",
    "\n",
    "from src.compute_metrics import Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train / test data\n",
    "We can use the datapreprocessing method for that.\n",
    "\n",
    "This is done for better reproducibility, but can be done with the sklearn train / test split, and setting a seed should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = dp.get_train_test_split(dp.X)\n",
    "y_train, y_test = dp.get_train_test_split(dp.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for training\n",
    "We will use k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost_and_get_metrics(X, y, \n",
    "                                    catboost_params = {},\n",
    "                                    backward_transform_label=True, \n",
    "                                    backward_standardize_flag=False, verbose=False,\n",
    "                                    standard_scale_flag=False,\n",
    "                                    impute_flag=False):\n",
    "\n",
    "    if impute_flag:\n",
    "        imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "        X = pd.DataFrame(imp_mean.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    bst = CatBoostRegressor(**catboost_params)\n",
    "\n",
    "    metrics = Metrics(dp=dp, backward_transform_flag=backward_transform_label, backward_standardize_flag=backward_standardize_flag)\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        if standard_scale_flag:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "            X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "\n",
    "        bst.fit(X_train, y_train)\n",
    "        y_pred = bst.predict(X_val)\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"y_pred: {y_pred[:5]}\")\n",
    "            print(f\"y_val: {y_val[:5]}\")\n",
    "            \n",
    "        computed_metrics = metrics.get_single_train_val_metrics(bst, X_train, y_train, X_val, y_val)\n",
    "        metrics.append(computed_metrics)\n",
    "\n",
    "    average_metrics = metrics.get_average()\n",
    "    std_metrics = metrics.get_std()\n",
    "    # Add _std to the keys to differentiate them from the average metrics:\n",
    "    std_metrics = {f\"{key}_std\" : value for key, value in std_metrics.items()} \n",
    "\n",
    "    return {**average_metrics, **std_metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define convenience functions for prettier display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_metrics(metrics_dict, only_validation=True, format_mean_std_together=True):\n",
    "\n",
    "    if only_validation:\n",
    "        metrics_dict = {key: value for key, value in metrics_dict.items() if \"test_\" in key}\n",
    "\n",
    "    if format_mean_std_together:\n",
    "        metrics_dict = {key: f\"{value:.2f} ± {metrics_dict[key+'_std']:.2f}\" for key, value in metrics_dict.items() if \"std\" not in key}\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.replace(\"nan ± nan\", \"0\").apply(lambda x: x.split(\"+-\")[0]).max()\n",
    "    return ['font-weight: bold' if v else '' for v in is_max]\n",
    "\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.replace(\"nan ± nan\", \"0\").apply(lambda x: x.split(\"+-\")[0]).min()\n",
    "    return ['font-weight: bold' if v else '' for v in is_min]\n",
    "\n",
    "def format_results_df(results, column_names=None):\n",
    "    results_df = pd.DataFrame(results).T\n",
    "\n",
    "    if column_names is not None:\n",
    "        results_df.columns = column_names\n",
    "    \n",
    "    def apply_highlight(column):\n",
    "        if column.name in [\"test_explained_variance\", \"test_r2\", \"test_custom_1\", \"test_custom_5\", \"test_custom_10\", \"test_custom_20\"]:\n",
    "            return highlight_max(column)\n",
    "        else:\n",
    "            return highlight_min(column)\n",
    "\n",
    "    \n",
    "    return results_df.style.apply(apply_highlight, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out different preprocessing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_catboost_without_impute_without_standard = train_catboost_and_get_metrics(X_train, y_train, \n",
    "                                                                catboost_params = {\"verbose\" : False},\n",
    "                                                                impute_flag=False,\n",
    "                                                                standard_scale_flag=False)\n",
    "                                                                \n",
    "results_catboost_with_impute_without_standard = train_catboost_and_get_metrics(X_train, y_train, \n",
    "                                                                catboost_params = {\"verbose\" : False},\n",
    "                                                                impute_flag=True,\n",
    "                                                                standard_scale_flag=False)\n",
    "\n",
    "results_catboost_without_impute_with_standard = train_catboost_and_get_metrics(X_train, y_train, \n",
    "                                                                catboost_params = {\"verbose\" : False},\n",
    "                                                                impute_flag=False,\n",
    "                                                                standard_scale_flag=True)\n",
    "                                                                \n",
    "results_catboost_with_impute_with_standard = train_catboost_and_get_metrics(X_train, y_train,\n",
    "                                                                catboost_params = {\"verbose\" : False},\n",
    "                                                                impute_flag=True,\n",
    "                                                                standard_scale_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_1383a_row0_col0, #T_1383a_row0_col1, #T_1383a_row0_col2, #T_1383a_row0_col3, #T_1383a_row1_col0, #T_1383a_row1_col1, #T_1383a_row1_col2, #T_1383a_row1_col3, #T_1383a_row2_col2, #T_1383a_row3_col2, #T_1383a_row4_col2, #T_1383a_row5_col0, #T_1383a_row5_col2, #T_1383a_row6_col2, #T_1383a_row7_col2, #T_1383a_row8_col0, #T_1383a_row9_col2 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1383a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1383a_level0_col0\" class=\"col_heading level0 col0\" >No impute, no standard</th>\n",
       "      <th id=\"T_1383a_level0_col1\" class=\"col_heading level0 col1\" >Impute, no standard</th>\n",
       "      <th id=\"T_1383a_level0_col2\" class=\"col_heading level0 col2\" >No impute, standard</th>\n",
       "      <th id=\"T_1383a_level0_col3\" class=\"col_heading level0 col3\" >Impute, standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row0\" class=\"row_heading level0 row0\" >test_explained_variance</th>\n",
       "      <td id=\"T_1383a_row0_col0\" class=\"data row0 col0\" >0.64 ± 0.09</td>\n",
       "      <td id=\"T_1383a_row0_col1\" class=\"data row0 col1\" >0.64 ± 0.09</td>\n",
       "      <td id=\"T_1383a_row0_col2\" class=\"data row0 col2\" >0.64 ± 0.09</td>\n",
       "      <td id=\"T_1383a_row0_col3\" class=\"data row0 col3\" >0.64 ± 0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row1\" class=\"row_heading level0 row1\" >test_r2</th>\n",
       "      <td id=\"T_1383a_row1_col0\" class=\"data row1 col0\" >0.64 ± 0.09</td>\n",
       "      <td id=\"T_1383a_row1_col1\" class=\"data row1 col1\" >0.64 ± 0.09</td>\n",
       "      <td id=\"T_1383a_row1_col2\" class=\"data row1 col2\" >0.64 ± 0.09</td>\n",
       "      <td id=\"T_1383a_row1_col3\" class=\"data row1 col3\" >0.64 ± 0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row2\" class=\"row_heading level0 row2\" >test_mape</th>\n",
       "      <td id=\"T_1383a_row2_col0\" class=\"data row2 col0\" >221.98 ± 83.79</td>\n",
       "      <td id=\"T_1383a_row2_col1\" class=\"data row2 col1\" >220.33 ± 90.18</td>\n",
       "      <td id=\"T_1383a_row2_col2\" class=\"data row2 col2\" >209.04 ± 88.19</td>\n",
       "      <td id=\"T_1383a_row2_col3\" class=\"data row2 col3\" >217.00 ± 95.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row3\" class=\"row_heading level0 row3\" >test_median_absolute_error</th>\n",
       "      <td id=\"T_1383a_row3_col0\" class=\"data row3 col0\" >447.64 ± 3.49</td>\n",
       "      <td id=\"T_1383a_row3_col1\" class=\"data row3 col1\" >447.73 ± 3.01</td>\n",
       "      <td id=\"T_1383a_row3_col2\" class=\"data row3 col2\" >447.13 ± 3.25</td>\n",
       "      <td id=\"T_1383a_row3_col3\" class=\"data row3 col3\" >448.51 ± 3.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row4\" class=\"row_heading level0 row4\" >test_mean_absolute_error</th>\n",
       "      <td id=\"T_1383a_row4_col0\" class=\"data row4 col0\" >682.07 ± 7.28</td>\n",
       "      <td id=\"T_1383a_row4_col1\" class=\"data row4 col1\" >683.21 ± 6.90</td>\n",
       "      <td id=\"T_1383a_row4_col2\" class=\"data row4 col2\" >681.84 ± 8.31</td>\n",
       "      <td id=\"T_1383a_row4_col3\" class=\"data row4 col3\" >683.19 ± 7.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row5\" class=\"row_heading level0 row5\" >test_mean_squared_log_error</th>\n",
       "      <td id=\"T_1383a_row5_col0\" class=\"data row5 col0\" >0.12 ± 0.02</td>\n",
       "      <td id=\"T_1383a_row5_col1\" class=\"data row5 col1\" >0.12 ± 0.03</td>\n",
       "      <td id=\"T_1383a_row5_col2\" class=\"data row5 col2\" >0.12 ± 0.02</td>\n",
       "      <td id=\"T_1383a_row5_col3\" class=\"data row5 col3\" >0.12 ± 0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row6\" class=\"row_heading level0 row6\" >test_custom_1</th>\n",
       "      <td id=\"T_1383a_row6_col0\" class=\"data row6 col0\" >10.22 ± 0.29</td>\n",
       "      <td id=\"T_1383a_row6_col1\" class=\"data row6 col1\" >10.16 ± 0.23</td>\n",
       "      <td id=\"T_1383a_row6_col2\" class=\"data row6 col2\" >10.32 ± 0.13</td>\n",
       "      <td id=\"T_1383a_row6_col3\" class=\"data row6 col3\" >10.24 ± 0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row7\" class=\"row_heading level0 row7\" >test_custom_5</th>\n",
       "      <td id=\"T_1383a_row7_col0\" class=\"data row7 col0\" >45.51 ± 0.35</td>\n",
       "      <td id=\"T_1383a_row7_col1\" class=\"data row7 col1\" >45.54 ± 0.22</td>\n",
       "      <td id=\"T_1383a_row7_col2\" class=\"data row7 col2\" >45.66 ± 0.43</td>\n",
       "      <td id=\"T_1383a_row7_col3\" class=\"data row7 col3\" >45.42 ± 0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row8\" class=\"row_heading level0 row8\" >test_custom_10</th>\n",
       "      <td id=\"T_1383a_row8_col0\" class=\"data row8 col0\" >72.07 ± 0.15</td>\n",
       "      <td id=\"T_1383a_row8_col1\" class=\"data row8 col1\" >71.98 ± 0.13</td>\n",
       "      <td id=\"T_1383a_row8_col2\" class=\"data row8 col2\" >72.06 ± 0.15</td>\n",
       "      <td id=\"T_1383a_row8_col3\" class=\"data row8 col3\" >71.93 ± 0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1383a_level0_row9\" class=\"row_heading level0 row9\" >test_custom_20</th>\n",
       "      <td id=\"T_1383a_row9_col0\" class=\"data row9 col0\" >92.02 ± 0.30</td>\n",
       "      <td id=\"T_1383a_row9_col1\" class=\"data row9 col1\" >92.06 ± 0.20</td>\n",
       "      <td id=\"T_1383a_row9_col2\" class=\"data row9 col2\" >92.14 ± 0.20</td>\n",
       "      <td id=\"T_1383a_row9_col3\" class=\"data row9 col3\" >92.05 ± 0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x162b593f1a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = pd.concat([pd.DataFrame(filter_metrics(results_catboost_without_impute_without_standard, only_validation=True, format_mean_std_together=True), index=[\"Without Impute Without Standard\"]),\n",
    "                        pd.DataFrame(filter_metrics(results_catboost_with_impute_without_standard, only_validation=True, format_mean_std_together=True), index=[\"With Impute Without Standard\"]),\n",
    "                        pd.DataFrame(filter_metrics(results_catboost_without_impute_with_standard, only_validation=True, format_mean_std_together=True), index=[\"Without Impute With Standard\"]),\n",
    "                        pd.DataFrame(filter_metrics(results_catboost_with_impute_with_standard, only_validation=True, format_mean_std_together=True), index=[\"With Impute With Standard\"])])\n",
    "\n",
    "format_results_df(df_results, column_names=[\"No impute, no standard\", \"Impute, no standard\", \"No impute, standard\", \"Impute, standard\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are quite similar, but no imputing and standardizing seems to have the best results.\n",
    "\n",
    "However, for simplicity, we go for no imputing and no standardizing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
